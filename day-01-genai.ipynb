{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:41.540239Z","iopub.execute_input":"2024-12-11T05:28:41.540647Z","iopub.status.idle":"2024-12-11T05:28:41.547356Z","shell.execute_reply.started":"2024-12-11T05:28:41.540611Z","shell.execute_reply":"2024-12-11T05:28:41.546228Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:41.559173Z","iopub.execute_input":"2024-12-11T05:28:41.559593Z","iopub.status.idle":"2024-12-11T05:28:51.888657Z","shell.execute_reply.started":"2024-12-11T05:28:41.559547Z","shell.execute_reply":"2024-12-11T05:28:51.887054Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import google.generativeai as genai\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:51.891757Z","iopub.execute_input":"2024-12-11T05:28:51.892273Z","iopub.status.idle":"2024-12-11T05:28:51.897980Z","shell.execute_reply.started":"2024-12-11T05:28:51.892211Z","shell.execute_reply":"2024-12-11T05:28:51.896709Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:51.899404Z","iopub.execute_input":"2024-12-11T05:28:51.900030Z","iopub.status.idle":"2024-12-11T05:28:52.120067Z","shell.execute_reply.started":"2024-12-11T05:28:51.899962Z","shell.execute_reply":"2024-12-11T05:28:52.119004Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"flash = genai.GenerativeModel('gemini-1.5-flash')\nresponse = flash.generate_content(\"Explain AI to me like I'm a kid.\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:52.122980Z","iopub.execute_input":"2024-12-11T05:28:52.123453Z","iopub.status.idle":"2024-12-11T05:28:53.728580Z","shell.execute_reply.started":"2024-12-11T05:28:52.123403Z","shell.execute_reply":"2024-12-11T05:28:53.727155Z"}},"outputs":[{"name":"stdout","text":"Imagine you have a super smart puppy.  You teach it tricks, like \"sit\" and \"fetch.\"  At first, it's clumsy, but the more you teach it and show it examples, the better it gets!\n\nArtificial Intelligence (AI) is like that super smart puppy, but instead of tricks, it learns from information – lots and lots of information!  We give it pictures, words, sounds, and numbers, and it learns patterns and how to do things.  \n\nSometimes it learns to recognize cats in pictures, sometimes it learns to translate languages, and sometimes it even learns to play games better than humans!\n\nIt's not actually *thinking* like you and me, it's just really good at finding patterns and following instructions based on what it's learned.  It's a clever computer program that's getting smarter all the time!\n\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:53.730072Z","iopub.execute_input":"2024-12-11T05:28:53.730399Z","iopub.status.idle":"2024-12-11T05:28:53.737922Z","shell.execute_reply.started":"2024-12-11T05:28:53.730366Z","shell.execute_reply":"2024-12-11T05:28:53.736586Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Imagine you have a super smart puppy.  You teach it tricks, like \"sit\" and \"fetch.\"  At first, it's clumsy, but the more you teach it and show it examples, the better it gets!\n\nArtificial Intelligence (AI) is like that super smart puppy, but instead of tricks, it learns from information – lots and lots of information!  We give it pictures, words, sounds, and numbers, and it learns patterns and how to do things.  \n\nSometimes it learns to recognize cats in pictures, sometimes it learns to translate languages, and sometimes it even learns to play games better than humans!\n\nIt's not actually *thinking* like you and me, it's just really good at finding patterns and following instructions based on what it's learned.  It's a clever computer program that's getting smarter all the time!\n"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"chat = flash.start_chat(history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:53.739167Z","iopub.execute_input":"2024-12-11T05:28:53.739490Z","iopub.status.idle":"2024-12-11T05:28:54.229284Z","shell.execute_reply.started":"2024-12-11T05:28:53.739457Z","shell.execute_reply":"2024-12-11T05:28:54.228148Z"}},"outputs":[{"name":"stdout","text":"It's nice to meet you, Zlork!  How can I help you today?\n\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"response = chat.send_message('Can you tell something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:54.231104Z","iopub.execute_input":"2024-12-11T05:28:54.231531Z","iopub.status.idle":"2024-12-11T05:28:55.373418Z","shell.execute_reply.started":"2024-12-11T05:28:54.231473Z","shell.execute_reply":"2024-12-11T05:28:55.372274Z"}},"outputs":[{"name":"stdout","text":"Some dinosaurs had feathers!  While the image of scaly dinosaurs is ingrained in our minds,  many theropod dinosaurs, the group that includes *Tyrannosaurus rex*, were actually covered in feathers, at least partially.  These feathers weren't always like the feathers of modern birds, some were more like fuzzy down, but their presence is strong evidence supporting the evolutionary link between dinosaurs and birds.\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# While you have the `chat` object around, the conversation state\n# persists. Confirm that by asking if it knows my name.\nresponse = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:55.374748Z","iopub.execute_input":"2024-12-11T05:28:55.375039Z","iopub.status.idle":"2024-12-11T05:28:55.769000Z","shell.execute_reply.started":"2024-12-11T05:28:55.375010Z","shell.execute_reply":"2024-12-11T05:28:55.767961Z"}},"outputs":[{"name":"stdout","text":"Yes, your name is Zlork.\n\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"for model in genai.list_models():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:55.770332Z","iopub.execute_input":"2024-12-11T05:28:55.770632Z","iopub.status.idle":"2024-12-11T05:28:55.891313Z","shell.execute_reply.started":"2024-12-11T05:28:55.770601Z","shell.execute_reply":"2024-12-11T05:28:55.890110Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro\nmodels/gemini-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-pro-exp-0801\nmodels/gemini-1.5-pro-exp-0827\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-exp-0827\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/learnlm-1.5-pro-experimental\nmodels/gemini-exp-1114\nmodels/gemini-exp-1121\nmodels/gemini-exp-1206\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/aqa\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"for model in genai.list_models():\n  if model.name == 'models/gemini-1.5-flash':\n    print(model)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:55.895104Z","iopub.execute_input":"2024-12-11T05:28:55.895452Z","iopub.status.idle":"2024-12-11T05:28:55.989444Z","shell.execute_reply.started":"2024-12-11T05:28:55.895410Z","shell.execute_reply":"2024-12-11T05:28:55.988344Z"}},"outputs":[{"name":"stdout","text":"Model(name='models/gemini-1.5-flash',\n      base_model_id='',\n      version='001',\n      display_name='Gemini 1.5 Flash',\n      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n      input_token_limit=1000000,\n      output_token_limit=8192,\n      supported_generation_methods=['generateContent', 'countTokens'],\n      temperature=1.0,\n      max_temperature=2.0,\n      top_p=0.95,\n      top_k=40)\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"short_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(max_output_tokens=200))\n\nresponse = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:55.990899Z","iopub.execute_input":"2024-12-11T05:28:55.991334Z","iopub.status.idle":"2024-12-11T05:28:57.616878Z","shell.execute_reply.started":"2024-12-11T05:28:55.991286Z","shell.execute_reply":"2024-12-11T05:28:57.615727Z"}},"outputs":[{"name":"stdout","text":"## The Enduring Significance of Olives in Modern Society\n\nThe olive, *Olea europaea*, a seemingly humble fruit, holds a position of remarkable importance in modern society, extending far beyond its culinary applications.  Its significance is woven into the fabric of history, culture, economy, and even health, demonstrating a multifaceted influence that continues to resonate in the 21st century. This essay will explore the diverse ways in which olives contribute to the global landscape, highlighting their economic impact, cultural significance, and burgeoning role in health and wellness.\n\nEconomically, the olive and its derivative products form a cornerstone of several national economies.  The Mediterranean region, particularly Greece, Spain, Italy, and Tunisia, is the heartland of olive cultivation, with olive oil production accounting for a significant portion of their agricultural GDP.  These countries have developed sophisticated olive oil industries, encompassing everything from cultivation and harvesting techniques to processing, packaging, and marketing. This industry supports millions of livelihoods, from farmers and agricultural\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:57.618403Z","iopub.execute_input":"2024-12-11T05:28:57.618867Z","iopub.status.idle":"2024-12-11T05:28:58.446007Z","shell.execute_reply.started":"2024-12-11T05:28:57.618818Z","shell.execute_reply":"2024-12-11T05:28:58.444933Z"}},"outputs":[{"name":"stdout","text":"From ancient groves, a bounty springs,\nThe olive's gift, on sun-kissed wings.\nIn oil it flows, a kitchen's grace,\nA vibrant taste, in every place.\n\nFrom salad bowls to hearty bread,\nIts richness shared, its virtues spread.\nA symbol strong, of health and might,\nThe olive shines, a welcome light.\n\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"from google.api_core import retry\n\nhigh_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=2.0))\n\n\n# When running lots of queries, it's a good practice to use a retry policy so your code\n# automatically retries when hitting Resource Exhausted (quota limit) errors.\nretry_policy = {\n    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n}\n\nfor _ in range(5):\n  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                              request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:28:58.447273Z","iopub.execute_input":"2024-12-11T05:28:58.447655Z","iopub.status.idle":"2024-12-11T05:29:00.236416Z","shell.execute_reply.started":"2024-12-11T05:28:58.447619Z","shell.execute_reply":"2024-12-11T05:29:00.235234Z"}},"outputs":[{"name":"stdout","text":"Aquamarine\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"low_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=0.0))\n\nfor _ in range(5):\n  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                             request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:00.237845Z","iopub.execute_input":"2024-12-11T05:29:00.238261Z","iopub.status.idle":"2024-12-11T05:29:01.937255Z","shell.execute_reply.started":"2024-12-11T05:29:00.238214Z","shell.execute_reply":"2024-12-11T05:29:01.936102Z"}},"outputs":[{"name":"stdout","text":"Maroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        # These are the default values for gemini-1.5-flash-001.\n        temperature=1.0,\n        top_k=64,\n        top_p=0.95,\n    ))\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = model.generate_content(story_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:01.939036Z","iopub.execute_input":"2024-12-11T05:29:01.939472Z","iopub.status.idle":"2024-12-11T05:29:04.563160Z","shell.execute_reply.started":"2024-12-11T05:29:01.939423Z","shell.execute_reply":"2024-12-11T05:29:04.562052Z"}},"outputs":[{"name":"stdout","text":"Bartholomew, a ginger tabby with a heart as big as his rumbling purr, was bored. The usual routine of napping in sunbeams, chasing dust motes, and demanding belly rubs from his human, Emily, had grown tiresome. He yearned for adventure, something more exciting than the occasional rogue sock or the tantalizing scent of tuna from the kitchen. \n\nOne day, as Emily was leaving for work, Bartholomew saw his chance. He slipped out the back door, a mischievous glint in his golden eyes. The world outside was a vibrant tapestry of smells, sounds, and sights. He stalked through the garden, his whiskers twitching with excitement, sniffing every bush and tree.\n\nHe soon encountered a group of alley cats, their sleek bodies and wary eyes.  Bartholomew, despite his privileged upbringing, possessed a natural charm. He shared his stash of kibble, gained their trust, and learned of a legendary place: the Whispering Wall, a forgotten alleyway where, legend said, wishes whispered on the wind came true.\n\nIntrigued, Bartholomew set off with the alley cats, guided by the setting sun. The journey was perilous, fraught with dangers: a ferocious Rottweiler, a grumpy old woman with a broom, and a particularly cunning squirrel who stole his tuna treat. But Bartholomew, spurred on by his desire for adventure and his new feline friends, persevered.\n\nFinally, they reached the Whispering Wall, a towering brick structure draped in ivy. The air hummed with an ancient energy, and Bartholomew could feel it in his bones. He closed his eyes, took a deep breath, and whispered his wish: \"I wish for a world filled with endless treats and belly rubs.\"\n\nThe wind carried his wish, and Bartholomew, feeling a surge of hope, opened his eyes. The alley cats, sensing the power of the wall, also whispered their wishes. \n\nThe next morning, Bartholomew returned home, his whiskers twitching with satisfaction. Emily, surprised to see him, greeted him with a warm hug and a plate of delicious cat food.  Bartholomew, full of contentment, curled up on her lap, a purr rumbling deep in his chest.  He had had his adventure, and his wish, though not entirely fulfilled, had brought him a newfound appreciation for the simple joys of home. And that, he decided, was an adventure in itself. \n\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=5,\n    ))\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:04.564593Z","iopub.execute_input":"2024-12-11T05:29:04.564938Z","iopub.status.idle":"2024-12-11T05:29:04.832245Z","shell.execute_reply.started":"2024-12-11T05:29:04.564904Z","shell.execute_reply":"2024-12-11T05:29:04.830962Z"}},"outputs":[{"name":"stdout","text":"Sentiment: **POSITIVE**\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ))\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:04.833520Z","iopub.execute_input":"2024-12-11T05:29:04.833859Z","iopub.status.idle":"2024-12-11T05:29:05.352734Z","shell.execute_reply.started":"2024-12-11T05:29:04.833827Z","shell.execute_reply":"2024-12-11T05:29:05.351709Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ))\n\nfew_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\n\nresponse = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:05.354485Z","iopub.execute_input":"2024-12-11T05:29:05.354974Z","iopub.status.idle":"2024-12-11T05:29:06.057495Z","shell.execute_reply.started":"2024-12-11T05:29:05.354923Z","shell.execute_reply":"2024-12-11T05:29:06.056340Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n  \"size\": \"large\",\n  \"type\": \"normal\",\n  \"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ))\n\nresponse = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:06.058754Z","iopub.execute_input":"2024-12-11T05:29:06.059098Z","iopub.status.idle":"2024-12-11T05:29:06.610624Z","shell.execute_reply.started":"2024-12-11T05:29:06.059065Z","shell.execute_reply":"2024-12-11T05:29:06.609467Z"}},"outputs":[{"name":"stdout","text":"{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert pizza\"}\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nresponse = model.generate_content(prompt, request_options=retry_policy)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:06.612045Z","iopub.execute_input":"2024-12-11T05:29:06.612365Z","iopub.status.idle":"2024-12-11T05:29:06.911606Z","shell.execute_reply.started":"2024-12-11T05:29:06.612326Z","shell.execute_reply":"2024-12-11T05:29:06.910237Z"}},"outputs":[{"name":"stdout","text":"44\n\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = model.generate_content(prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:06.913019Z","iopub.execute_input":"2024-12-11T05:29:06.913430Z","iopub.status.idle":"2024-12-11T05:29:08.206835Z","shell.execute_reply.started":"2024-12-11T05:29:06.913383Z","shell.execute_reply":"2024-12-11T05:29:08.205635Z"}},"outputs":[{"name":"stdout","text":"Step 1: Find the partner's age when you were 4.\n\n* When you were 4, your partner was 3 times your age, so they were 3 * 4 = 12 years old.\n\nStep 2: Find the age difference between you and your partner.\n\n* The age difference is 12 - 4 = 8 years.\n\nStep 3: Determine your partner's current age.\n\n* You are now 20 years old.\n* Your partner is 8 years older than you.\n* Therefore, your partner is currently 20 + 8 = 28 years old.\n\nSo the answer is $\\boxed{28}$\n\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"\n\n# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:08.208445Z","iopub.execute_input":"2024-12-11T05:29:08.208781Z","iopub.status.idle":"2024-12-11T05:29:08.215776Z","shell.execute_reply.started":"2024-12-11T05:29:08.208738Z","shell.execute_reply":"2024-12-11T05:29:08.214634Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nreact_chat = model.start_chat()\n\n# You will perform the Action, so generate up to, but not including, the Observation.\nconfig = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n\nresp = react_chat.send_message(\n    [model_instructions, example1, example2, question],\n    generation_config=config,\n    request_options=retry_policy)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:08.217365Z","iopub.execute_input":"2024-12-11T05:29:08.217746Z","iopub.status.idle":"2024-12-11T05:29:09.788402Z","shell.execute_reply.started":"2024-12-11T05:29:08.217693Z","shell.execute_reply":"2024-12-11T05:29:09.787271Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the Transformers NLP paper and then find the authors and their ages.  This will require multiple steps.  First, I need to find the paper.\n\nAction 1\n<search>Transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:09.789955Z","iopub.execute_input":"2024-12-11T05:29:09.790294Z","iopub.status.idle":"2024-12-11T05:29:11.232242Z","shell.execute_reply.started":"2024-12-11T05:29:09.790261Z","shell.execute_reply":"2024-12-11T05:29:11.231125Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nThe observation gives me the authors of the paper \"Attention is All You Need\". I now need to find their ages.  This is not directly provided, and may be impossible to find definitively.  I may need to settle for estimating ages based on publication date and likely career stage.\n\nAction 2\n<lookup>publication date</lookup>\n\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ))\n\n# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = model.generate_content(code_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:11.233833Z","iopub.execute_input":"2024-12-11T05:29:11.234271Z","iopub.status.idle":"2024-12-11T05:29:11.948215Z","shell.execute_reply.started":"2024-12-11T05:29:11.234216Z","shell.execute_reply":"2024-12-11T05:29:11.947094Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```\n"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"def factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:11.949484Z","iopub.execute_input":"2024-12-11T05:29:11.949843Z","iopub.status.idle":"2024-12-11T05:29:11.955202Z","shell.execute_reply.started":"2024-12-11T05:29:11.949809Z","shell.execute_reply":"2024-12-11T05:29:11.954070Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    tools='code_execution',)\n\ncode_exec_prompt = \"\"\"\nCalculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n\"\"\"\n\nresponse = model.generate_content(code_exec_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:11.956389Z","iopub.execute_input":"2024-12-11T05:29:11.956797Z","iopub.status.idle":"2024-12-11T05:29:14.298547Z","shell.execute_reply.started":"2024-12-11T05:29:11.956764Z","shell.execute_reply":"2024-12-11T05:29:14.297409Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"To calculate the sum of the first 14 odd prime numbers, I need to first identify those primes.  I will use Python to generate the primes and then calculate the sum.\n\n\n``` python\ndef is_prime(n):\n    \"\"\"Checks if a number is prime.\"\"\"\n    if n <= 1:\n        return False\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\n\ncount = 0\nnumber = 1\nsum_of_primes = 0\nwhile count < 14:\n    number += 2 # start from 3 and only check odd numbers\n    if is_prime(number):\n        sum_of_primes += number\n        count += 1\n\nprint(f\"The sum of the first 14 odd prime numbers is: {sum_of_primes}\")\n\n\n```\n```\nThe sum of the first 14 odd prime numbers is: 326\n\n```\nTherefore, the sum of the first 14 odd prime numbers is 326.\n"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"def is_prime(num):\n  \"\"\"Checks if a number is prime.\"\"\"\n  if num <= 1:\n    return False\n  for i in range(2, int(num**0.5) + 1):\n    if num % i == 0:\n      return False\n  return True\n\nprimes = []\ncount = 0\nnum = 2\nwhile count < 14:\n  if is_prime(num) and num % 2 != 0:\n    primes.append(num)\n    count += 1\n  num += 1\n\nprint(f'The first 14 odd primes are: {primes}')\nprint(f'Their sum is: {sum(primes)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:14.303111Z","iopub.execute_input":"2024-12-11T05:29:14.303445Z","iopub.status.idle":"2024-12-11T05:29:14.311921Z","shell.execute_reply.started":"2024-12-11T05:29:14.303414Z","shell.execute_reply":"2024-12-11T05:29:14.310571Z"}},"outputs":[{"name":"stdout","text":"The first 14 odd primes are: [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nTheir sum is: 326\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n  print(part)\n  print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:14.313571Z","iopub.execute_input":"2024-12-11T05:29:14.314036Z","iopub.status.idle":"2024-12-11T05:29:14.330825Z","shell.execute_reply.started":"2024-12-11T05:29:14.313970Z","shell.execute_reply":"2024-12-11T05:29:14.329667Z"}},"outputs":[{"name":"stdout","text":"text: \"To calculate the sum of the first 14 odd prime numbers, I need to first identify those primes.  I will use Python to generate the primes and then calculate the sum.\\n\\n\"\n\n-----\nexecutable_code {\n  language: PYTHON\n  code: \"\\ndef is_prime(n):\\n    \\\"\\\"\\\"Checks if a number is prime.\\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\ncount = 0\\nnumber = 1\\nsum_of_primes = 0\\nwhile count < 14:\\n    number += 2 # start from 3 and only check odd numbers\\n    if is_prime(number):\\n        sum_of_primes += number\\n        count += 1\\n\\nprint(f\\\"The sum of the first 14 odd prime numbers is: {sum_of_primes}\\\")\\n\\n\"\n}\n\n-----\ncode_execution_result {\n  outcome: OUTCOME_OK\n  output: \"The sum of the first 14 odd prime numbers is: 326\\n\"\n}\n\n-----\ntext: \"Therefore, the sum of the first 14 odd prime numbers is 326.\\n\"\n\n-----\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\n\nresponse = model.generate_content(explain_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:29:14.332206Z","iopub.execute_input":"2024-12-11T05:29:14.332582Z","iopub.status.idle":"2024-12-11T05:29:17.036177Z","shell.execute_reply.started":"2024-12-11T05:29:14.332538Z","shell.execute_reply":"2024-12-11T05:29:17.035112Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a bash script that enhances your terminal prompt to display information about your current Git repository.  It's essentially a custom prompt theme for Git.\n\n**What it does:**\n\nAt a high level, the script adds information to your command prompt, such as:\n\n* **Branch name:**  The name of the current Git branch.\n* **Status indicators:** Symbols indicating changes (added, modified, deleted, etc.) in your repository.\n* **Upstream status:** Information about whether your local branch is ahead of or behind the remote branch.\n* **Virtual environment:** If you are using a virtual environment (like conda or virtualenv), it will display the environment name.\n* **Customizable colors and themes:** The script allows you to customize the colors and appearance of the prompt using themes.\n\n\n**Why you'd use it:**\n\nYou would use this script if you want a more informative and visually appealing command prompt when working with Git.  Instead of a plain prompt, you get a summary of your repository's state at a glance, improving your workflow.  The customization options let you tailor the appearance to your preferences.  The script also handles edge cases like detached HEAD states and various Git commands gracefully.\n\n\nIn short: it's a tool to make your Git experience more efficient and visually pleasant within your terminal.\n"},"metadata":{}}],"execution_count":60}]}